---
title: "Adaptive MCMC Output"
author: "Hillary Heiling"
date: "November 13, 2019"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Description of datasets and output of interest:

Of the datasets from the Pajor vs Glmer simulation results (glmer_vs_Pajor_diagnostics.R file, giving glmer_Pajor_diagnostics.RData) all had 5 groups, 3 variables (intercept + 2 slopes), and N = 500. Of these datasets, the datasets simulated to have sd_ranef = 1.0 were selected, and of those, the datasets that further had generally low acceptance rates were selected (at least 6 of the 5*3=15 acceptance rates were less than 0.25). Of the original 100 datasets with sd_ranef = 1.0, 37 had generally low acceptance rates as described.

For each of these datasets, fit_dat was re-run, and 3 additional E steps with M = 20,000 were run. These additional E steps used the following sampling schemes: the original Metropolis-within-Gibbs with proposal variance fixed at 1.0, the adaptive Metropolis-within-Gibbs, and the Metropolis-within-Gibbs with the ability to fix the proposal variance at some value other than 1.0. In this last fixed version, the proposal standard deviation was set at 1.5 for original acceptance rates < 0.35, 1/1.5 for original acceptance rates above 0.55, and 1.0 otherwise.

Of these 37 dataset outputs of interest, we will examine the first 7 for the moment just to get an idea of what is going on.

```{r}
library(stringr)
library(ggplot2)
library(reshape2)

# Load adaptive_vs_set_mcmc_abbrev.RData object
# Subset of adaptive_vs_set_mcmc_noplots.RData, which is the same as 
# adaptive_vs_set_mcmc.RData but without the plots
# Gives test_results_abbrev object
# prefix = getwd()
load("adaptive_vs_set_mcmc_abbrev2.RData")
output = test_results_abbrev2
samples = output
```

```{r}
## Evaluate the performance of the chain
mcmc_diagnostics = function(post_U){ # modified version of plot_mcmc.pglmmObj
  
  d = 5
  var_num = 3
  
  type = c("sample.path","histogram","cumsum","autocorr")
  
  U_keep = post_U
  var_names = c("Intercept","X1","X2")
  grp_names = str_c("grp", 1:5)
  var_str = rep(var_names, each = d)
  grp_str = rep(grp_names, times = var_num)
  U_cols = str_c(var_str, ":", grp_str)
  
  vars = "all"
  grps = "all"
  
  U_t = data.frame(U_keep, t = 1:nrow(U_keep))
  colnames(U_t) = c(U_cols, "t")
  U_long = melt(U_t, id = "t")
  U_plot = data.frame(U_long, var_names = rep(var_names, each = d*nrow(U_keep)),
                      grp_names = rep(rep(grp_names, each = nrow(U_keep)), times = var_num))
  
  plots_return = list()
  
  if("sample.path" %in% type){
    plot_sp = ggplot(U_plot, mapping = aes(x = t, y = value)) + geom_path() +
      facet_grid(var_names ~ grp_names) + xlab("iteration t") + ylab("draws")
    
    plots_return$sample_path = plot_sp
  }
  if("histogram" %in% type){
    hist_U = ggplot(U_plot) + geom_histogram(mapping = aes(x = value)) + 
      facet_grid(var_names ~ grp_names) + xlab("draws")
    
    plots_return$histogram = hist_U
  }
  if("cumsum" %in% type){
    U_means = colMeans(U_keep)
    U_means = data.frame(rbind(U_means))[rep.int(1L, nrow(U_keep)), , drop = FALSE]
    U_tmeans = apply(U_keep, 2, cumsum) / 1:nrow(U_keep)
    U_tdiff = U_tmeans - U_means
    U_cumsum = apply(U_tdiff, 2, cumsum)
    U_t = data.frame(U_cumsum, t = 1:nrow(U_cumsum))
    colnames(U_t) = c(colnames(U_keep), "t")
    U_long = melt(U_t, id = "t")
    U_plot = data.frame(U_long, var_names = rep(var_names, each = d*nrow(U_keep)),
                        grp_names = rep(rep(grp_names, each = nrow(U_keep)), times = var_num)) 
    plot_cumsum = ggplot(U_plot) + geom_smooth(mapping = aes(x = t, y = value), color = "black") +
      geom_hline(yintercept = 0, linetype = "dashed") +
      facet_grid(var_names ~ grp_names) + xlab("iteration t") + ylab("Cumulative Sum")
    
    plots_return$cumsum = plot_cumsum
  }
  if("autocorr" %in% type){
    grp_index = rep(grp_names, times = var_num)
    var_index = rep(var_names, each = d)
    for(j in 1:ncol(U_keep)){
      ACF = acf(U_keep[,j], plot=F, lag.max = 40)
      ACF_df = with(ACF, data.frame(lag,acf))
      ACF_df$grp_names = grp_index[j]
      ACF_df$var_names = var_index[j]
      if(j == 1){
        ACF_all = ACF_df
      }else{
        ACF_all = rbind(ACF_all, ACF_df)
      }
    }
    
    plot_acf = ggplot(data = ACF_all, mapping = aes(x = lag, y = acf)) +
      geom_hline(mapping = aes(yintercept = 0)) + 
      geom_segment(mapping = aes(xend = lag, yend = 0)) +
      facet_grid(var_names ~ grp_names)
    
    plots_return$autocorr = plot_acf
  }
  
  return(plots_return)
  
}

```


```{r}
original_plots = list()
manual_plots = list()
adaptive_plots = list()

for(i in 1:length(samples)){
  
  original_plots[[i]] = mcmc_diagnostics(samples[[i]]$original$post_U)
  manual_plots[[i]] = mcmc_diagnostics(samples[[i]]$manual$post_U)
  adaptive_plots[[i]] = mcmc_diagnostics(samples[[i]]$adaptive$post_U)
  
}
```

Let's take a look at the output from the first 5 datasets to start with:

## Output from "Original" Metropolis-within-Gibbs (proposal variance set at 1.0)

Original sample paths:

```{r}

original_plots[[1]]$sample_path
original_plots[[2]]$sample_path
original_plots[[3]]$sample_path
original_plots[[4]]$sample_path
original_plots[[5]]$sample_path
```

Summary: None of the variables or groups have any disturbing pattern trends (no linear increases or other functional patterns over iteration number ... ). However, the X2 variable sample paths have very "wide" sample paths in the 3rd - 5th dataset output compared to the other variables.  

Original autocorrelation plots:

```{r}
original_plots[[1]]$autocorr
original_plots[[2]]$autocorr
original_plots[[3]]$autocorr
original_plots[[4]]$autocorr
original_plots[[5]]$autocorr
```

Summary: In general, nothing about these autocorrelation plots look too worrying to me.

## Output from Adaptive Metropolis-within-Gibbs 

Adaptive sample paths:

```{r}
adaptive_plots[[1]]$sample_path
adaptive_plots[[2]]$sample_path
adaptive_plots[[3]]$sample_path
adaptive_plots[[4]]$sample_path
adaptive_plots[[5]]$sample_path
```

Summary: Again, the 3rd - 5th dataset outputs have some weird behavior for the X2 variable. There are obvious periods of no mixing well into the 20,000 iterations.

Adaptive autocorrelation plots:

```{r}
adaptive_plots[[1]]$autocorr
adaptive_plots[[2]]$autocorr
adaptive_plots[[3]]$autocorr
adaptive_plots[[4]]$autocorr
adaptive_plots[[5]]$autocorr
```

Summary: Unlike in the original posterior draw distribution, here there is some significant autocorrelation for the X2 variable.

Note: If you would like to look at the plots from the manual changing of the proposal standard deviatoin / variance, you could look at the sample_path and/or the autocorr plots from the manual_plots[[i]] list. 

## Acceptance Rate Output 

General variable assignments

```{r}
d = 5
q = 3

vars = rep(c("Intercept","X1","X2"), each = d)
grps = rep(1:d, times = q)
labels = str_c(vars, ":", grps)

```

Original

```{r}

original_rate_matrix = matrix(0, nrow = length(samples), ncol = d*q)
colnames(original_rate_matrix) = labels

for(i in 1:length(samples)){
  original_rate_matrix[i,] = c(samples[[i]]$original$gibbs_accept_rate)
}

```

Adaptive

```{r}

adaptive_rate_matrix = matrix(0, nrow = length(samples), ncol = d*q)
colnames(adaptive_rate_matrix) = labels

for(i in 1:length(samples)){
  adaptive_rate_matrix[i,] = c(samples[[i]]$adaptive$gibbs_accept_rate)
}


```

Manual

```{r}

manual_rate_matrix = matrix(0, nrow = length(samples), ncol = d*q)
colnames(manual_rate_matrix) = labels

for(i in 1:length(samples)){
  manual_rate_matrix[i,] = c(samples[[i]]$manual$gibbs_accept_rate)
}
```


### Comparing the original acceptance rates with the adaptive acceptance rates rates

Let x = original acceptance rate, y = difference between adaptive and original acceptance rates

Plot the relationship:

```{r}
x = c(original_rate_matrix)
y = c(adaptive_rate_matrix - original_rate_matrix)

plot(x, y, xlab = "original rates", ylab = "adaptive - original rates",
     main = "Comparison of acceptance rates")
abline(h = 0)
```

Summary: 

In the region where the original acceptance rates are lower than we would like (i.e. 0.40 and below), the adaptive Metropolis-within-Gibbs scheme generally lowers the acceptance rate instead of increasing it, as we would prefer.

On the other hand, when the acceptance rates are too high, the adaptive scheme does tend to lower the acceptance rates, although in some cases, perhaps by too much.

Let x = original acceptance rate, y = adaptive acceptance rates

Plot the relationship:

```{r}
x = c(original_rate_matrix)
y = c(adaptive_rate_matrix)

plot(x, y, xlab = "original rates", ylab = "adaptive rates",
     main = "Comparison of acceptance rates", ylim = c(0,1))
abline(a = 0, b = 1)

```

### Comparing the original acceptance rates with the manual acceptance rates rates

Recall: When the original acceptance rates were below 0.35, I manually set the proposal standard deviation to be 1.5. When the accptance rates were above 0.55, I manually set the proposal standard deviation to be 1/1.5. For all other "middle" acceptance rates, I left the proposal standard deviation as 1.0. 

Consequently, we would expect that for all cases when the original accepance rate was < 0.35, we would expect the new acceptance rate to be generally larger when we manually increased the proposal standard deviation. Similarly, when the origainl acceptance rate was > 0.55, we would expect the new acceptance rate to be generally smaller when we manually decreased the proposal standard deviation.

Let x = original acceptance rate, y = difference between adaptive and original acceptance rates

Plot the relationship:

```{r}
x = c(original_rate_matrix)
y = c(manual_rate_matrix - original_rate_matrix)

plot(x, y, xlab = "original rates", ylab = "manual - original rates",
     main = "Comparison of acceptance rates")
abline(h = 0)
abline(v = c(0.35,0.55))

```

Summary: We see for some of the really extreme cases (original acceptance rates < 0.1 and near 1), the manual changes had the anticipated effect. For some of the intermediate cases (acceptance rates around 0.2, for instance), manually changing the proposal standard deviation had the opposite effect. This could perhaps be because the change from SD = 1.0 to SD = 1.5 was too drastic a change for that region?

Let x = original acceptance rate, y = manual acceptance rates

Plot the relationship:

```{r}
x = c(original_rate_matrix)
y = c(manual_rate_matrix)

plot(x, y, xlab = "original rates", ylab = "manual rates",
     main = "Comparison of acceptance rates", ylim = c(0,1))
abline(a = 0, b = 1)

```

## Ending Proposal Standard Deviation for Adaptive Process

Note: The cap of the proposal standard deviation was (exp(-1), exp(1))

```{r}
proposal_SD = matrix(0, nrow = length(samples), ncol = d*q)

for(i in 1:length(samples)){
  # proposal_SD[i,] = c(samples[[i]]$adaptive$proposal_var)
  proposal_SD[i,] = c(samples[[i]]$adaptive$proposal_SD)
}

plot(x = c(original_rate_matrix), y = c(proposal_SD), ylim = c(0, exp(1)+0.1),
     main = ("original acceptance rates vs ending proposal SD"),
     ylab = "Ending Proposal SD", xlab = "Original Acceptance Rates")
abline(h = c(exp(-1),exp(1)))
```

Summary: It looks like once the acceptance rate was already low, increasing the proposal variance / SD didn't increase the acceptance rates (or at least not enough to reach the desired area of (0.4, 0.5)), so the proposal SD / variance kept increasing with no obvious effect.


The End