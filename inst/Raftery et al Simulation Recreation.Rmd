---
title: "Simulation Recreation"
author: "Hillary Heiling"
date: "September 8, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 2 Example

Recreation of one of the simulations in Section 2 of Raftery et al. (Estimating the integrating likelihood via posterior simulation using the harmonic mean identity)

Description of simulation: We are interested in the marginal likelihood for a single data point y

$$ y \sim N(\mu, \frac{1}{\psi}) $$

$$ \psi \sim Gamma(\alpha / 2, \alpha / 2) $$

Paper prior specification:
$$ \mu|\psi \sim N(\mu_0, n_0 \psi) $$

Correction to make things conjugate (I think), as the paper claims these priors are:

$$ \mu|\psi \sim N(\mu_0, n_0 \psi^{-1}) $$

Integrated likelihood analytical result:

$$ f(y) = St(y|\mu_0,\lambda = n_0/(n_0+1),\alpha) = \frac{\Gamma(\frac{1}{2}(\alpha+1))}{\Gamma(\frac{1}{2}\alpha)} \left ( \frac{\lambda}{\alpha \pi} \right)^{1/2} * [1 + \alpha^{-1} \lambda (y-\mu_0)^2]^{-(\alpha+1)/2} $$

Let the hyperparameters have the following values:

$ \mu_0 = 0, \alpha = 2, n_0 = 1 $

Target value:

```{r}
alpha = 2
mu = 0
y = 5
n0 = 1
lambda = n0 / (1+n0)

(f_y = gamma(0.5*(alpha+1)) / gamma(0.5*alpha) * (lambda / alpha / pi)^0.5 * 
  (1 + 1/alpha*lambda*(y-mu)^2)^((-1)*(alpha+1)/2))

# In harmonic mean functions, will shoot for following target value:
1/f_y

```

Note: Raftery et al. also used the following combinations: y = {5,3,0}, alpha = {2,6,10}, $\mu_0$ remained equal to 0.

## Gathering the MCMC samples

Gibbs approach: Sample $\psi$ from appropriate gamma dist and then sample $\mu$ based on given $\psi$ sample. ($\psi$ does not depend on $\mu$, so can independently sample $\psi$ from the specified gamma dist).

$$ \psi^{-1}|y \sim IG(\alpha/2, \alpha/2) \rightarrow \psi|y \sim Gamma(\alpha/2,\alpha/2) $$

Note: In general for multiple data points:

$$ \psi|y_1,...,y_n \sim Gamma(\alpha/2 + n/2 - 1/2, \alpha/2 + 1/2 \sum (y_i - \bar y)^2) $$
$$ \mu | \psi,y \sim N \left (\frac{y}{2}, \frac{\psi^{-1}}{2} \right) $$

Note: In general for multiple data points:

$$ \mu | \psi,y \sim N \left (\frac{n \bar y + \mu_0}{n + 1}, \frac{\psi^{-1}}{n + 1} \right) $$

```{r}
draws = function(mu0, alpha, M, y){
  
  psi = rgamma(M, shape = alpha / 2, scale = alpha / 2)
  mu = rnorm(M, mean = y/2, sd = sqrt(1/psi/2))
  
  return(cbind(mu,psi))
}
```


## Evaluating the Modified and Regular Harmonic Mean estimates using the MCMC samples

Once we have the MCMC samples, we can plug them into the (modified) harmonic mean functions. Returned value should be near target value given above.

```{r}
HM_sect2 = function(MCMC, y){
  mu = MCMC[,1]
  psi = MCMC[,2] # precision of y; inverse of variance of y
  sd_y = sqrt(1/psi)
  dens = dnorm(y, mean = mu, sd = sd_y)
  inv_dens = 1 / dens
  
  # marg_lik = 1 / mean(inv_dens)
  # ll = log(marg_lik)
  
  return(mean(inv_dens))
}

mod_HM_sect2 = function(MCMC, y, mu0, alpha, p){
  mu = MCMC[,1]
  psi = MCMC[,2]
  sd_y = sqrt(1/psi)
  
  # Calculation of denominator 
  dens = dnorm(y, mean = mu, sd = sd_y)
  prior = dnorm(mu, mean = mu0, sd = sqrt(1/psi)) * dgamma(psi, shape = alpha/2, scale = alpha/2)
  
  # Calculation of numerator
  # Posterior modes and covariance:
  mode = colMeans(MCMC)
  sigma = cov(MCMC)
  # Calculation of truncated normal:
  wt_norm = dmvnorm(MCMC, mean = mode, sigma = sigma)
  region = apply(MCMC, 1, function(x) t(x - mode) %*% solve(sigma) %*% (x - mode))
  cutoff = qchisq(p = p, df = 2) # df = number parameters, mu and psi
  trunc_norm = ifelse(region > cutoff, 0, wt_norm)
  print("proportion truncated:")
  print(sum(region > cutoff) / length(region))
  
  if(anyNA(trunc_norm)) print(sum(is.na(trunc_norm)))
  
  components = trunc_norm / (dens * prior)
  inv_lik = mean(components)
  # ll = log(1/inv_lik)
  
  return(inv_lik)
}
```

Results:

```{r}
library(mvtnorm)
mcmc_samps = draws(mu0 = 0, alpha = 2, 10^4, y = 5)

HM_sect2(mcmc_samps, 0)
mod_HM_sect2(mcmc_samps, y = 5, mu0 = 0, alpha = 2, p = 0.98)
```

# Section 4.2 Example

y distribution: 

$$ y_1,...,y_n \sim N(\mu,I) $$
$$ \bar y \sim N(\mu, I/n) $$ 

Prior for mean:

$$ \mu \sim N(0,I) $$
Posterior for mean:

$$ \mu|y \sim N \left ( \frac{n \bar y}{n+1}, \frac{I}{n+1} \right ) $$
True log integrated likelihood value:

$$ \ell(\bar y) = log(f(\bar y)) = \frac{d}{2} log \left( \frac{n}{(n+1)2\pi} \right ) - \frac{n}{2(n+1)} \sum_{j=1}^d \bar y_j^2 $$

Let us assign the following values to the hyperparameters:

$ \mu = 0.15, n = 50, d = 3 $

## Evaluate Target Log-Likelihood log(f(y)) and Related Values

The function:

```{r}

ll = function(d, n, y_bar){
  ll = d/2 * log(n/(2*pi*(n+1))) - n/(2*(n+1)) * sum(y_bar^2)
}

```

## Sample y and posterior samples of $\mu$

Also, evaluate target loglik

```{r}
library(mvtnorm)
mu0 = 0.15 # for all dimensions of y
n = 100
d = 3
I = diag(x = 1, nrow = d)
M = 10^4

set.seed(1)
y_bar = rmvnorm(n = 1, mean = rep(mu0, times = d), sigma = I/n)

post_mu = rmvnorm(n = M, mean = n*y_bar/(n+1), sigma = I/(n+1))

# Target log-lik:
target = ll(d, n, y_bar)
target # log-lik
# exp(target) # marginal/integrated likelihood
# 1/exp(target) # inverse of marginal likelihood
```

## (Modified) Harmonic Mean Estimate

```{r}

HM = function(posterior, y){
  # Define variables
  d = length(y)
  M = nrow(posterior)
  I = diag(x = 1, nrow = d)
  
  ll = 0
  dens = numeric(M)
  # For each posterior draw u, calculate y|u density
  for(s in 1:M){
    dens[s] = dmvnorm(y_bar, mean = posterior[s,], sigma = I)
  }
  # Find inverse of densities
  inv_dens = 1/dens
  # Sum inverse densities
  inv_lik = mean(inv_dens)
  # Inverse of inverse-likelihood = likelihood
  lik = 1 / inv_lik
  # Sum log of individual likelihoods
  ll = log(lik)
  
  return(ll)
}

mod_HM = function(posterior, y, mu0, p){
  # Define variables
  # n = nrow(y)
  M = nrow(posterior)
  d = length(y)
  I = diag(x=1,nrow=d)
  
  ll = 0
  dens = numeric(M)
  
  # Calculate posterior modes and covariance (for numerator)
  post_means = colMeans(posterior)
  post_cov = cov(posterior)
  
  # Denominator component - prior evaluated at posterior sample values
  prior = dmvnorm(posterior, mean = rep(mu0, times = d), sigma = I)
  # Calculation of truncated normal - numerator
  wt_norm = dmvnorm(posterior, mean = post_means, sigma = post_cov)
  region = apply(posterior, 1, function(x) t(x - post_means) %*% solve(post_cov) %*% 
                   (x - post_means))
  cutoff = qchisq(p=p, df=d)
  trunc_norm = ifelse(region>cutoff, 0, wt_norm) / p
  print("proportion truncated:")
  print(sum(region>cutoff) / length(region))
  
  for(s in 1:M){
    dens[s] = dmvnorm(y, mean = posterior[s,], sigma = I)
  }
  
  # Calculate f* / (denity * prior)
  components = trunc_norm / (dens * prior)
  # Sum these components over all posterior draws to get inverse-likelihood 
  inv_lik = mean(components)
  # Find likelihood (inverse of inverse), then sum logs of these likelihoods
  ll = log(1/inv_lik)
  
  return(ll)
}

############# 
# for experimentation purposes only
# mod_HM_test = function(posterior, y, mu0, p){
#   # Define variables
#   # n = nrow(y)
#   M = nrow(posterior)
#   d = length(y)
#   I = diag(x=1,nrow=d)
#   
#   ll = 0
#   dens = numeric(M)
#   
#   # Calculate posterior modes and covariance (for numerator)
#   post_means = colMeans(posterior)
#   post_cov = cov(posterior)
#   
#   # Denominator component - prior evaluated at posterior sample values
#   prior = dmvnorm(posterior, mean = rep(mu0, times = d), sigma = I)
#   # Calculation of truncated normal - numerator
#   wt_norm = dmvnorm(posterior, mean = post_means, sigma = post_cov, log = F)
#   print(summary(wt_norm))
#   region = apply(posterior, 1, function(x) t(x - post_means) %*% solve(post_cov) %*% 
#                    (x - post_means))
#   cutoff = qchisq(p=p, df=d)
#   trunc_norm = ifelse(region>cutoff, 0, wt_norm) / p 
#   
#   print("proportion truncated:")
#   print(sum(region>cutoff) / length(region))
#   
#   for(s in 1:M){
#     dens[s] = dmvnorm(y, mean = posterior[s,], sigma = I)
#   }
#   
#   # Calculate f* / (denity * prior)
#   components = trunc_norm / (dens * prior)
#   print(summary(trunc_norm / dens / prior))
#   # Sum these components over all posterior draws to get inverse-likelihood 
#   inv_lik = mean(components)
#   # Find likelihood (inverse of inverse), then sum logs of these likelihoods
#   ll = log(1/inv_lik)
#   
#   return(ll)
# }

```

```{r}
HM(post_mu, y_bar)
mod_HM(post_mu, y_bar, mu0=0.15, p=0.97)

mod_HM_test(post_mu, y_bar, mu0 = 0.15, p = 0.98)
```

# Raftery et al Second Method - using 4.2 example

## Method:

We will be using the same y and $\mu | y$ set-up specified earlier for example 4.2.

Given B independent samples from the posterior distribution, we can calculate the log densities of y (or in this case, $\bar y$) when evaluated at each of the posterior samples (density of $ \bar y | \mu^t$ for posterior samples t = {1,...,M}). 

$$ \ell_t = log(f(\bar y| \mu^t)) = \frac{d}{2} log(n/(2 \pi)) - \frac{n}{2} \sum_{j=1}^d (\bar y_j - \mu_j^t)^2 $$

Using equation (20) in the article (pg. 15), we can find the following estimate of the log marginal likelihood (for fixed effects model):

$$ log(\hat f_{BICM}(\bar y)) = \bar \ell - s_{\ell}^2(log(n) - 1) $$
where the above quantities (sample mean and variance) are:

$$ \bar \ell = \frac{1}{M} \sum_{t=1}^M \ell_t $$
$$ s_{\ell}^2 = \frac{1}{M-1} \sum_{t=1}^M (\ell_t - \bar \ell)^2 $$

Note: The $ s_{\ell}^2 $ is supposed to be an estimate of $d/2$, where d is the true number of parameters. In this case, the number of parameters d should be the length of the mean vector (and the length of the $\bar y$ vector). Consequently, if d = 3, $s_{\ell}^2$ should be about 1.5.

## Code 

```{r}
library(mvtnorm)
mu0 = 0.15 # for all dimensions of y
n = 300
d = 17
I = diag(x = 1, nrow = d)
M = 10^5

set.seed(1)
y_bar = rmvnorm(n = 1, mean = rep(mu0, times = d), sigma = I/n)

post_mu = rmvnorm(n = M, mean = n*y_bar/(n+1), sigma = I/(n+1))

# Target log-lik:
target = ll(d, n, y_bar)
target # log-lik
```

```{r}

ll_BICM = function(posterior, y_bar, n){
  # Define variables
  d = length(y_bar)
  M = nrow(posterior)
  
  y_bar_mat = matrix(y_bar, nrow = 1)
  diff = (y_bar_mat[rep(1, times = M),] - posterior)
  
  ll_t = d/2 * log(n/(2*pi)) - n/2 * rowSums(diff^2)
  ll_bar = mean(ll_t)
  cat("ll_bar: ", ll_bar, "\n")
  # ll_var = var(ll_t)
  # cat("estimate of d:", ll_var*2, "; true d:", d, "\n")
  ll_var = d/2
  
  # BICM estimate of log integrated likelihood - equation (20) pg. 15
  ll_bicm = ll_bar - ll_var * (log(n) - 1)
  
  # lognormal estimate of the integrated likelihood - equation (29) pg. 18
  ## Paper said this could be useful when there is very high dimensionality in the 
  ## theta paramter of interest - for us, if alpha is high dimensional
  ll_ln = ll_bar - 0.5 * (ll_var)
  
  return(list(BICM = ll_bicm, LogNorm = ll_ln))
}

ll_BICM(post_mu, y_bar, n = n)

HM(post_mu, y_bar)

```

# Arithmetic Mean Approach (Anna Pajor) - Code by Naim

Assumption: using same example set-up as the 4.2 example.

## Code 

Set-up and target: 

```{r}
library(mvtnorm)
mu0 = 0.15 # for all dimensions of y
n = 300
d = 25
I = diag(x = 1, nrow = d)
M = 10^5

set.seed(1)
y_bar = rmvnorm(n = 1, mean = rep(mu0, times = d), sigma = I/n)

post_mu = rmvnorm(n = M, mean = n*y_bar/(n+1), sigma = I/(n+1))

# Target log-lik:
target = ll(d, n, y_bar)
target # log-lik
```

CAME estimate code:

```{r}
## Corrected Arithmetic mean estimator
# determine bounds as the max of each dim
bounds = apply(post_mu, 2, range) 
inbounds = function(x){mean(x > bounds[1,] & x < bounds[2,]) == 1}
# sample from prior
prior = rmvnorm(nrow(post_mu), sigma = I)
# calculate density and indicator
dens = apply(prior, 1, FUN = function(x){dmvnorm(y_bar, mean = x, sigma = I/n)})
ind = apply(prior, 1, inbounds)^2
mean(dens*ind) # regular CAME
log(mean(dens*ind))
# now try importance sampling
imp = rmvnorm(nrow(post_mu),mean = colMeans(post_mu) ,sigma = cov(post_mu))
# importance weighted densite
dens = apply(imp, 1, FUN = function(x){dmvnorm(y_bar, mean = x, sigma = I/n)})*dmvnorm(imp)/dmvnorm(imp,mean = colMeans(post_mu) ,sigma = cov(post_mu) )
ind = apply(imp, 1, inbounds)^2
mean(dens*ind) # IS CAME, much more accurate, use this one
log(mean(dens*ind)) 
```

# Naim's Code for Section 2 Example

```{r}
alpha = 2
mu = 0
y = 0
n_0 = 1
lambda = n_0 / (1+n_0)
M = 10000
# taken from https://statswithr.github.io/book/inference-and-decision-making-with-multiple-parameters.html#sec:NG-MC
# translated our parameters into theirs
m_0 = mu; n_0 = n_0;  s2_0 = 1; v_0 = alpha
# sample summaries
Y = y
ybar = mean(Y)
s2 = 0#var(Y)
n = length(Y)
# posterior hyperparamters
n_n = n_0 + n
m_n = (n*ybar + n_0*m_0)/n_n
v_n = v_0 + n
s2_n = ((n-1)*s2 + v_0*s2_0 + n_0*n*(m_0 - ybar)^2/n_n)/v_n
# drawing from the posterior, note that the source I am using assume rate instead of scale
psip  = rgamma(M, shape = v_0/2, rate = v_0/2)
mup = rnorm(M, mean = m_n, sd = sqrt(1/psip)/sqrt(n_n)) 
mcmc_samps2 = cbind(mup, psip)
mcmc_samps = draws(m_0, 2, M, y)
summary(mcmc_samps2)
summary(mcmc_samps) # from hillary's code, similar 
par(mfrow = c(2, 1))
hist(mcmc_samps2[,1], breaks = 1000, xlim = c(-5, 5))
hist(mcmc_samps[,1], breaks = 1000, xlim = c(-5, 5))
# calcull
lik2 = dnorm(Y,mean = mcmc_samps2[,1],sd = sqrt(1/mcmc_samps2[,2]))
lik = dnorm(Y,mean = mcmc_samps[,1],sd = sqrt(1/mcmc_samps[,2]))
#HM
mean(1/lik2)^-1
mean(1/lik)^-1
# Modified HM
1/mod_HM_sect2(mcmc_samps2, y = y, mu0 = mu, alpha = alpha, p = 0.98)
1/mod_HM_sect2(mcmc_samps, y = y, mu0 = mu, alpha = alpha, p = 0.98)

# truth
f_y
```




The End